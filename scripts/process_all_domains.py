"""
Master Corpus Pipeline
======================

This script orchestrates the processing of ALL 23 domains generated by the extractor.
It iterates through every .jsonl file in data/processed/domains and runs the 
GSW 6-Stage Extraction Pipeline on each one.

Usage:
    python scripts/process_all_domains.py

Options:
    --limit N   : Limit documents per domain (default: 100)
    --resume    : Resume processing from last checkpoint
"""

import argparse
import sys
import subprocess
import time
from pathlib import Path

# Add project root
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

DOMAINS_DIR = PROJECT_ROOT / "data" / "processed" / "domains"

def run_pipeline_on_file(domain_file: Path, limit: int, resume: bool = False, free: bool = False):
    """Run gsw_pipeline.py for a specific domain file."""
    domain_name = domain_file.stem # e.g., "family" from "family.jsonl"
    
    print(f"\n>>> PROCESSING DOMAIN: {domain_name.upper()} <<<")
    
    cmd = [
        sys.executable,
        "scripts/gsw_pipeline.py",
        "process",
        "--domain", domain_name,
        "--limit", str(limit)
    ]
    
    if resume:
        cmd.append("--resume")
        
    if free:
        cmd.append("--free")
        
    try:
        # Run as subprocess to ensure clean memory state for each domain
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] Failed to process {domain_name}: {e}")

def main():
    parser = argparse.ArgumentParser(description="Process all corpus domains")
    parser.add_argument("--limit", type=int, default=100, help="Docs per domain to process")
    parser.add_argument("--resume", action="store_true", help="Resume processing")
    parser.add_argument("--free", action="store_true", help="Use free model rotation")
    args = parser.parse_args()

    if not DOMAINS_DIR.exists():
        print(f"Error: Domains directory not found at {DOMAINS_DIR}")
        print("Has the corpus_domain_extractor finished?")
        sys.exit(1)

    # Get all .jsonl files except statistics
    domain_files = sorted([
        f for f in DOMAINS_DIR.glob("*.jsonl") 
        if "statistics" not in f.name
    ])

    if not domain_files:
        print("No domain files found. Is extraction complete?")
        sys.exit(1)

    print(f"Found {len(domain_files)} domain files to process.")
    print(f"Limit per domain: {args.limit}")
    print(f"Free Mode: {args.free}")
    print("-" * 50)

    for i, f in enumerate(domain_files, 1):
        # Skip empty files
        if f.stat().st_size == 0:
            print(f"[{i}/{len(domain_files)}] Skipping {f.name} (Empty)")
            continue

        print(f"[{i}/{len(domain_files)}] Starting {f.name}...")
        run_pipeline_on_file(f, args.limit, args.resume, args.free)
        time.sleep(2) # Brief pause

    print("\n" + "="*50)
    print("ALL DOMAINS PROCESSED")
    print("="*50)

if __name__ == "__main__":
    main()

