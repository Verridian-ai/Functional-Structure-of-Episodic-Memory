<div align="center">

<!-- Logo & Title -->
<img src="assets/images/verridian_logo.png" alt="Verridian AI" width="180">

# VERRIDIAN AI

### Created by Daniel Fleuren

**A Universal Cognitive Brain for Any Domain**

*Giving Language Models Human-Like Episodic Memory*

*Legal ‚Ä¢ Medical ‚Ä¢ Business ‚Ä¢ Personal Knowledge ‚Ä¢ Research ‚Ä¢ Any Data*

<br>

<!-- PIPELINE OVERVIEW -->
> **üîÑ COMPLETE DATA PIPELINE**: This system processes the **full Australian Legal Corpus (232,560 documents)**. The pipeline follows: **Step 1: Data Preparation & Classification** ‚Üí **Step 2: GSW Extraction** ‚Üí **Step 3: Query & Analysis**. [See the complete pipeline](#-system-pipeline-overview).

<br>

> **üìö NEW TO VERRIDIAN?** Explore our comprehensive **[Wiki Documentation](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki)** for detailed guides, architecture explanations, and module references. The Wiki provides in-depth coverage of every component.

<br>

<!-- Animated Badges Row 1 -->
[![arXiv](https://img.shields.io/badge/arXiv-2511.07587-b31b1b?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.07587)
[![CLAUSE](https://img.shields.io/badge/CLAUSE-arXiv:2511.00340-b31b1b?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.00340v1)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-Next.js_16-3178C6?style=for-the-badge&logo=typescript&logoColor=white)](ui/)

<!-- Badges Row 2 -->
[![PyTorch](https://img.shields.io/badge/PyTorch-TEM+VSA-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](src/tem/)
[![React](https://img.shields.io/badge/React-19-61DAFB?style=for-the-badge&logo=react&logoColor=black)](ui/)
[![Pydantic](https://img.shields.io/badge/Pydantic-V2-E92063?style=for-the-badge&logo=pydantic&logoColor=white)](src/logic/)
[![LangChain](https://img.shields.io/badge/LangChain-Tools-121212?style=for-the-badge)](src/agents/)

<br>

<!-- Performance Metrics Visual -->
| üéØ 85% F1 Score | üìâ 51% Token Reduction | ‚ö° 42x Faster | ‚úÖ 100% Success |
|:---:|:---:|:---:|:---:|
| vs [77% RAG](https://arxiv.org/abs/2511.07587) | ~3,587 tokens | 11.83ms response | Query completion |

<br>

<!-- LAW OS Banner -->
<img src="assets/images/Law OS Banner.jpg" alt="LAW OS - Giving Language Models Human-Like Episodic Memory" width="100%">

<br>

<!-- UI Screenshot -->
<img src="assets/images/OS SCreenshot.png" alt="LAW OS Interface - 3D Knowledge Graph" width="100%">

<br>

<!-- Quick Links -->
[üìñ Documentation](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki) ‚Ä¢
[üöÄ Quick Start](#-quick-start) ‚Ä¢
[üèó Architecture](#-architecture) ‚Ä¢
[üñ• UI Features](#-ui-features--screenshots) ‚Ä¢
[üî¨ Research Validation](#-research-backed-validation-clause-benchmark) ‚Ä¢
[üìä Benchmarks](#-performance) ‚Ä¢
[ü§ù Contributing](CONTRIBUTING.md)

---

> **üôè Standing on the Shoulders of Giants**
>
> This architecture integrates foundational research from neuroscience and cognitive science. We gratefully acknowledge:
>
> **[Tolman-Eichenbaum Machine](https://www.cell.com/cell/fulltext/S0092-8674(20)31388-X)** ‚Äî *Whittington, Muller, Mark, Chen, Barry, Burgess & Behrens* (Cell, 2020)
>
> **[Active Inference](https://direct.mit.edu/neco/article/29/1/1/8207/Active-Inference-A-Process-Theory)** ‚Äî *Friston, FitzGerald, Rigoli, Schwartenbeck & Pezzulo* (Neural Computation, 2017)
>
> **[Clone-Structured Cognitive Graphs](https://www.nature.com/articles/s41467-021-22559-5)** ‚Äî *George, Rikhye, Gothoskar, Guntupalli, Dedieu & L√°zaro-Gredilla* (Nature Communications, 2021)
>
> **[Hyperdimensional Computing](https://link.springer.com/article/10.1007/s12559-009-9009-8)** ‚Äî *Kanerva* (Cognitive Computation, 2009)
>
> **[Global Workspace Theory](https://bernardbaars.com/)** ‚Äî *Baars* (1988, 1997)

</div>

## üß† A Universal Cognitive Brain for Any Domain

Verridian AI is a **production-ready** legal intelligence system implementing a novel **brain-inspired cognitive architecture**. Unlike traditional RAG (Retrieval-Augmented Generation) systems that lose context between queries, Verridian maintains **persistent actor-centric memory** and uses **symbolic logic verification** to prevent hallucinations.

### üîç Why is this different from traditional RAG?

```mermaid
flowchart LR
    subgraph Traditional["Traditional RAG"]
        Q1[Query] --> S[Search Chunks] --> L1[LLM] --> R1[Response]
    end

    subgraph Verridian["Verridian AI"]
        Q2[Query] --> M[Memory GSW] --> A[Agency] --> V[VSA Logic] --> R2[Verified Response]
    end
```

| Feature | Traditional RAG | Verridian AI |
|---------|----------------|--------------|
| Memory | ‚ùå No memory between queries | ‚úÖ Persistent actor-centric memory |
| Entities | ‚ùå Lost each time | ‚úÖ Tracks 5,170+ actors across time |
| Hallucination | ‚ùå No verification | ‚úÖ Logic verification layer |
| Tokens | ‚ùå ~8,000 per query | ‚úÖ ~3,500 (56% reduction) |

### üìö Core Concepts

| Concept | Description |
|---------|-------------|
| **Actor-Centric Memory** | Information organized around entities (actors) rather than events |
| **Persistent Memory** | Memory maintained across multiple queries (vs stateless retrieval) |
| **Structural Separation** | Distinguishing case structure from factual content |
| **Gap Detection** | Identifying missing evidence before responding |
| **Logic Verification** | Anti-hallucination through symbolic reasoning |

---

## üß© How It Works: Deep Dive (For Everyone)

### üéØ The Big Picture: What Problem Are We Solving?

Imagine asking your AI assistant: *"What happened with John Smith's custody case?"*

**Traditional AI (RAG)** works like a library with amnesia:
- üìö Searches through documents every single time
- üîÑ Forgets everything after each question
- ü§∑ Can't connect information across documents
- üé≤ Sometimes "makes stuff up" (hallucinations)

**Verridian AI** works like a detective with perfect memory:
- üß† Remembers all the people, relationships, and events
- üîó Connects information across hundreds of cases
- üîç Knows what's missing before answering
- ‚úÖ Verifies facts before speaking

```mermaid
flowchart LR
    subgraph Problem["‚ùå Traditional AI"]
        Q1[Your Question] --> Search[Search Everything]
        Search --> Forget[Forget & Repeat]
        Forget --> Maybe[Maybe Correct?]
    end

    subgraph Solution["‚úÖ Verridian AI"]
        Q2[Your Question] --> Remember[Check Memory]
        Remember --> Connect[Connect the Dots]
        Connect --> Verify[Verify Facts]
        Verify --> Confident[Confident Answer]
    end
```

---

## üß† The Five Building Blocks (Explained Simply)

### üóÑÔ∏è 1. Global Semantic Workspace (GSW) ‚Äî The Memory Palace

**Technical Definition**: The Global Semantic Workspace is a persistent, actor-centric knowledge graph that stores extracted entities, their relationships, states, and temporal links across all processed documents.

**Simple Analogy - Your Brain's Filing Cabinet**: Imagine your brain's memory as a **giant filing cabinet**. Most AI systems organize files by **event** (what happened). Verridian organizes files by **person** (who was involved).

```mermaid
flowchart TB
    subgraph Traditional["üìÅ Traditional: Event-Based Filing"]
        E1[üìÑ Event: Marriage 2010]
        E2[üìÑ Event: Divorce 2020]
        E3[üìÑ Event: Custody Hearing 2021]
    end

    subgraph Verridian["üß† Verridian: Person-Based Filing"]
        P1[üë§ John Smith]
        P2[üë§ Jane Smith]
        P3[üë∂ Children]

        P1 --> |married 2010| P2
        P1 --> |divorced 2020| P2
        P1 --> |custody of| P3
        P2 --> |custody of| P3
    end
```

**Why Does This Matter?**

- **Traditional approach**: Ask "Who is John?" - AI has to search through every event to piece together the answer.
- **Verridian approach**: Ask "Who is John?" - The answer is already organized: "John Smith: Applicant, Father, married 2010, separated 2020, works as accountant..."

**What's Inside the GSW?**

| Component | What It Stores | Real Example |
|-----------|---------------|--------------|
| **Actors** | People, organizations, assets | "John Smith" (person), "Family Court" (org) |
| **States** | Conditions that change over time | "Married" ‚Üí "Separated" ‚Üí "Divorced" |
| **Verb Phrases** | Actions and events | "John filed application on March 15" |
| **Questions** | Things we might need to know | "When did separation occur?" |
| **Links** | Connections in time and space | "John and Jane were both present on Date X" |

**By the Numbers**: 5,170 actors tracked ‚Ä¢ 7,615 questions answerable ‚Ä¢ 646 temporal links

---

### üó∫Ô∏è 2. TEM Layer ‚Äî The Mental GPS

**Technical Definition**: The Tolman-Eichenbaum Machine (TEM) is a neural architecture inspired by the hippocampal formation that learns to separate structural knowledge from sensory details, enabling generalization across similar situations.

**Simple Analogy - Google Maps for Your Brain**: Think of TEM like **Google Maps for information**:
- **Grid Cells (MEC)** = The underlying map structure (streets, intersections)
- **Place Cells (HPC)** = Specific locations you remember (your home, office)
- **Sensory Input (LEC)** = What you see right now (the actual buildings)

```mermaid
flowchart TB
    subgraph RealWorld["üåç Real World Navigation"]
        Map[üó∫Ô∏è Map Structure<br/>Streets & Layout]
        Places[üìç Specific Places<br/>Your Home, Office]
        Eyes[üëÅÔ∏è What You See<br/>Buildings, Signs]

        Map --> Navigate[üöó Navigate!]
        Places --> Navigate
        Eyes --> Navigate
    end

    subgraph LegalWorld["‚öñÔ∏è Legal Case Navigation"]
        Structure[üìã Case Structure<br/>Application ‚Üí Hearing ‚Üí Order]
        Specifics[üìç This Case<br/>John vs Jane Smith]
        Facts[üìù Specific Facts<br/>Dates, Amounts, Names]

        Structure --> Understand[üß† Understand Case]
        Specifics --> Understand
        Facts --> Understand
    end
```

**The Neuroscience Behind It**:

| Brain Region | Function | Legal AI Equivalent |
|--------------|----------|---------------------|
| **Grid Cells** | Create abstract coordinate system | Case type patterns (custody, property, divorce) |
| **Place Cells** | Mark specific locations | Specific people, dates, amounts |
| **Border Cells** | Detect boundaries | Legal deadlines, jurisdictions |
| **Head Direction** | Know which way you're facing | Know where you are in case timeline |

---

### üîç 3. Active Inference ‚Äî The Smart Detective

**Technical Definition**: Active Inference is a framework from computational neuroscience where agents minimize "free energy" by either updating beliefs (perception) or taking actions (exploration) to reduce uncertainty about their environment.

**Simple Analogy - A Detective Who Knows What Questions to Ask**: Imagine a brilliant detective who:
1. Knows what they DON'T know yet
2. Asks the right questions to fill gaps
3. Updates their theory as new evidence arrives
4. Knows when they have enough evidence to be confident

```mermaid
flowchart TB
    subgraph Detective["üïµÔ∏è How a Detective Works"]
        Observe[üëÅÔ∏è Observe Scene]
        Hypothesis[üí≠ Form Hypothesis]
        Gap[‚ùì What's Missing?]
        Investigate[üîç Investigate Gap]
        Update[üîÑ Update Theory]
        Solve[‚úÖ Solve Case]

        Observe --> Hypothesis
        Hypothesis --> Gap
        Gap --> Investigate
        Investigate --> Update
        Update --> Gap
        Update --> Solve
    end

    subgraph Verridian["‚öñÔ∏è How Verridian Works"]
        Read[üìñ Read Case]
        Extract[üìã Extract Facts]
        Missing[‚ùì What's Unknown?]
        Search[üîç Find Missing Info]
        Refine[üîÑ Update Understanding]
        Answer[‚úÖ Confident Answer]

        Read --> Extract
        Extract --> Missing
        Missing --> Search
        Search --> Refine
        Refine --> Missing
        Refine --> Answer
    end
```

**The Two Types of "Energy" It Minimizes**:

| Energy Type | What It Means | Example |
|-------------|---------------|---------|
| **Variational Free Energy** (VFE) | How surprised am I by what I see? | "I expected a separation date but didn't find one - that's surprising!" |
| **Expected Free Energy** (EFE) | What action will reduce my uncertainty the most? | "I should look in paragraph 3 for dates" |

<details>
<summary><strong>üî¨ Deep Dive: The Generative Model Matrices (A/B/C/D)</strong></summary>

| Matrix | Name | Function |
|--------|------|----------|
| **A** | Observation Likelihood | P(observation \| hidden state) - Maps states to observations |
| **B** | Transition Dynamics | P(next state \| current state, action) - How actions change states |
| **C** | Preferences | Log preferences over observations - What the agent "wants" to see |
| **D** | Prior Beliefs | P(initial state) - Starting beliefs before evidence |

</details>

---

### üõ°Ô∏è 4. VSA Layer ‚Äî The Fact-Checker

**Technical Definition**: Vector Symbolic Architecture (VSA) uses high-dimensional vectors (D=10,000) with three operations‚Äîbinding, bundling, and permutation‚Äîto represent and verify symbolic relationships in a way that's robust to noise and supports similarity-based reasoning.

**Simple Analogy - A Lie Detector for Information**: Imagine a super-powered fact-checker that can instantly verify if statements are consistent with everything it knows:

```mermaid
flowchart LR
    subgraph Input["üì• Statement Comes In"]
        Claim["John and Jane were married in 2010"]
    end

    subgraph VSA["üõ°Ô∏è VSA Verification"]
        Encode[üî¢ Convert to Math]
        Compare[‚öñÔ∏è Compare with Known Facts]
        Score[üìä Similarity Score]
    end

    subgraph Output["üì§ Verdict"]
        High["‚úÖ 0.95 - Verified!"]
        Low["‚ùå 0.23 - Contradiction!"]
    end

    Input --> Encode
    Encode --> Compare
    Compare --> Score
    Score --> High
    Score --> Low
```

**The Three Magic Operations**:

| Operation | Symbol | What It Does | Analogy |
|-----------|--------|--------------|---------|
| **Binding** | ‚äó | Connects two concepts | Tying two ideas together with a knot |
| **Bundling** | Œ£ | Combines multiple things | Putting items in the same bag |
| **Permutation** | œÅ | Creates sequences/order | Numbering items 1st, 2nd, 3rd |

**Anti-Hallucination Results**:

| Scenario | Without VSA | With VSA |
|----------|-------------|----------|
| Catches factual errors | ~60% | ~95% |
| False alarms | 15% | 3% |
| Response confidence | Unknown | Quantified (0-1 score) |

---

### üìù 5. TOON Format ‚Äî The Efficient Messenger

**Technical Definition**: Token-Oriented Object Notation (TOON) is a compact serialization format optimized for LLM context efficiency, achieving ~40% token reduction compared to JSON while maintaining 73.9% parsing accuracy.

**Simple Analogy - Text Messaging vs. Formal Letters**: When you text a friend, you don't write a formal letter. You write: `@ coffee main st. come hang?` ‚Äî TOON does the same thing for AI communication!

```mermaid
flowchart LR
    subgraph JSON["üìú JSON (Formal Letter)"]
        J1["{ 'actors': [
          {'id': 'a1', 'name': 'John'},
          {'id': 'a2', 'name': 'Jane'}
        ]}"]
    end

    subgraph TOON["üì± TOON (Text Message)"]
        T1["Actors[2]{id,name}
a1,John
a2,Jane"]
    end

    JSON --> |Same info, 40% smaller| TOON
```

**Cost Impact**:

| Metric | JSON | TOON | Savings |
|--------|------|------|---------|
| **Tokens per actor** | 45 | 27 | 40% |
| **Cost per 1000 queries** | $50 | $30 | $20 |
| **Context space used** | 100% | 60% | 40% more room for actual data |

---

## üé≠ How All Five Work Together

```mermaid
flowchart TB
    subgraph Input["üìÑ Input: Legal Document"]
        DOC[Court Judgment PDF]
    end

    subgraph GSW["üóÑÔ∏è GSW: Memory Palace"]
        direction TB
        Extract[Extract People, Events, Dates]
        Store[Store in Actor-Centric Format]
        Link[Link Everything Together]
    end

    subgraph TEM["üó∫Ô∏è TEM: Mental GPS"]
        direction TB
        Structure[Recognize Case Structure]
        Navigate[Navigate to Relevant Parts]
        Separate[Separate Facts from Structure]
    end

    subgraph Agency["üîç Active Inference: Detective"]
        direction TB
        Check[Check What's Missing]
        Decide[Decide What to Look For]
        Update[Update Understanding]
    end

    subgraph VSA["üõ°Ô∏è VSA: Fact-Checker"]
        direction TB
        Encode[Encode All Claims]
        Verify[Verify Consistency]
        Score[Calculate Confidence]
    end

    subgraph TOON["üìù TOON: Messenger"]
        direction TB
        Compress[Compress Everything]
        Efficient[Send Efficiently]
    end

    subgraph Output["‚úÖ Output"]
        Answer[Verified Answer<br/>with Confidence Score]
    end

    DOC --> GSW
    GSW --> TEM
    TEM --> Agency
    Agency --> VSA
    GSW <--> TOON
    TEM <--> TOON
    Agency <--> TOON
    VSA --> Output
```

### üèÉ The Journey of a Question

Let's follow what happens when you ask: *"When did John and Jane separate?"*

| Step | System | What Happens |
|------|--------|--------------|
| 1 | **GSW** | Finds "John Smith" and "Jane Smith" actors in memory |
| 2 | **TEM** | Recognizes this is a timeline question, navigates to relationship states |
| 3 | **Active Inference** | Checks: "Do I have separation date?" ‚Üí Yes! "June 2020" |
| 4 | **VSA** | Verifies: "June 2020" consistent with other dates? ‚Üí ‚úÖ Score: 0.95 |
| 5 | **TOON** | Compresses context throughout for efficiency |
| 6 | **Output** | "John and Jane separated in June 2020" (Confidence: 95%) |

---

## üèó Architecture

<div align="center">

### Three-Layer Cognitive System

```mermaid
flowchart TB
    subgraph Layer1["Layer 1: Navigation - TEM"]
        MEC[MEC<br/>Grid Cells] --> HPC[HPC<br/>Memory Binding]
        LEC[LEC<br/>Sensory] --> HPC
    end

    subgraph Layer2["Layer 2: Agency - Active Inference"]
        VFE[Variational Free Energy<br/>Perception] --> Decision[Action Selection]
        EFE[Expected Free Energy<br/>Exploration] --> Decision
    end

    subgraph Layer3["Layer 3: Logic - VSA"]
        BIND[Binding<br/>A ‚äó B] --> VERIFY[Verification]
        BUNDLE[Bundling<br/>Œ£ V] --> VERIFY
        PERMUTE[Permute<br/>œÅ V] --> VERIFY
    end

    Layer1 --> Layer2
    Layer2 --> Layer3
    Layer3 --> OUTPUT[Verified Response]
```

| Layer | Component | Function | Implementation |
|-------|-----------|----------|----------------|
| **1. Navigation** | TEM | Separates STRUCTURE from FACTS | `src/tem/model.py` |
| **2. Agency** | Active Inference | Detects missing evidence | `src/agency/agent.py` |
| **3. Logic** | VSA (D=10,000) | Anti-hallucination verification | `src/vsa/legal_vsa.py` |

</div>

### üîÑ Data Flow Through the System

```mermaid
flowchart TB
    DOC[Legal Document] --> CHUNK

    subgraph Ingestion["Ingestion Layer"]
        CHUNK[Text Chunker] --> OP[Legal Operator<br/>6 Tasks]
        OP --> REC[Reconciler]
    end

    REC --> GSW

    subgraph GSW["Global Semantic Workspace"]
        ACTORS[(Actors<br/>5,170)]
        QUESTIONS[(Questions<br/>7,615)]
        LINKS[(Links<br/>646)]
    end

    GSW --> TEM

    subgraph Engine["Three-Layer Cognitive Engine"]
        TEM[TEM<br/>Navigate] --> AGENCY[Agency<br/>Gap Check]
        AGENCY --> VSA[VSA<br/>Verify]
    end

    VSA --> RESPONSE[Verified Response<br/>Confidence: 0.95]
```

**6 Extraction Tasks**: Actor ID ‚Üí Roles ‚Üí States ‚Üí Verbs ‚Üí Questions ‚Üí Links

### üí° Core Innovation: Actor-Centric Memory

<div align="center">
<img src="assets/images/RAG VS GSW.png" alt="RAG vs GSW Comparison" width="100%">
</div>

Traditional NLP uses **verb-centric triples**: `(Subject, Verb, Object)`

Verridian uses **actor-centric memory** - organizing information around entities:

```python
# Traditional Verb-Centric (loses context)
("John", "married", "Jane")        # Who is John? Lost.
("John", "filed", "Application")   # Same John? Unknown.

# Verridian Actor-Centric (maintains context)
Actor: {
    name: "John Smith",
    type: "PERSON",
    roles: ["applicant", "husband", "father"],
    states: [
        {"name": "MaritalStatus", "value": "married", "when": "2010"},
        {"name": "MaritalStatus", "value": "separated", "when": "2020"}
    ],
    relationships: ["Jane Smith", "Children", "Family Home"],
    timeline: {"2010": "married", "2020": "separated", "2023": "filed"}
}
```

This mirrors how humans actually remember - achieving **85% accuracy** vs 77% for traditional RAG.

---

## üñ• UI Features & Screenshots

<div align="center">

### Modern Next.js 16 / React 19 Frontend

The Verridian AI interface is built with the latest web technologies, featuring a responsive, intuitive design that makes complex cognitive operations accessible. Built on Next.js 16's App Router with React 19, the frontend provides real-time interactions with the brain-inspired backend architecture.

> **Note**: The live UI demo uses mock data for demonstration purposes. To use with real legal data, complete the 6-step pipeline below to process your corpus.

</div>

### üí¨ Chat Interface

#### Main Chat Interface
<img src="assets/images/features/01-main-chat-interface.png" alt="Main Chat Interface" width="100%">

The primary interface for interacting with Verridian AI. Features a clean, distraction-free design with quick access to settings, canvas, and voice controls.

#### Active Conversation
<img src="assets/images/features/02-chat-with-messages.png" alt="Chat with Messages" width="100%">

Real-time chat with the GSW-powered AI system. Messages display actor-centric memory retrieval, TEM navigation insights, and VSA verification scores. Each response includes confidence metrics and references to source actors in the knowledge graph.

### ‚öôÔ∏è Admin Settings

#### System Prompt Configuration
<img src="assets/images/features/03-settings-panel-prompt.png" alt="Settings Panel - Prompt" width="100%">

Configure the core system prompts that guide the AI's behavior. Customize how the system interprets legal queries, applies active inference, and formats responses.

#### Model Configuration
<img src="assets/images/features/04-settings-model-config.png" alt="Settings - Model Config" width="100%">

Select and configure LLM models from OpenRouter. Supports all major providers including Claude Sonnet 4.5, GPT-4.1, Gemini 2.5 Pro/Flash, and specialized models like GLM-4.6 and Kimi K2. Configure temperature, top-p, and context window settings.

#### Tools Configuration
<img src="assets/images/features/05-settings-tools-config.png" alt="Settings - Tools Config" width="100%">

Enable/disable LangChain tools and cognitive modules. Control which agents have access to GSW queries, TEM navigation, VSA verification, document generation, and external integrations.

#### MCP (Model Context Protocol) Configuration
<img src="assets/images/features/06-settings-mcp-config.png" alt="Settings - MCP Config" width="100%">

Configure Model Context Protocol servers for extended capabilities. Connect to external knowledge sources, databases, and specialized tools through standardized MCP interfaces.

### üìÑ Canvas & Documents

#### Canvas Panel
<img src="assets/images/features/07-canvas-panel.png" alt="Canvas Panel" width="100%">

Visual workspace for exploring case structures, actor relationships, and temporal links. Provides a graphical representation of the GSW knowledge graph with interactive navigation through the TEM layer.

#### Document Generation Workflow
<img src="assets/images/features/10-document-generation-workflow.png" alt="Document Generation Workflow" width="100%">

Automated legal document generation using the GSW knowledge base. Create briefs, summaries, and case analyses with actor-centric information automatically populated from persistent memory.

### üé§ Voice Input

#### Voice Input Active
<img src="assets/images/features/08-voice-input-active.png" alt="Voice Input Active" width="100%">

Real-time voice input with speech-to-text integration. Ask questions naturally and receive spoken responses. Voice panel shows active listening status, transcription progress, and audio waveform visualization.

### üåê 3D Visualization

#### Knowledge Graph Visualization
<img src="assets/images/features/09-3d-visualization.png" alt="3D Visualization" width="100%">

Interactive 3D visualization of the GSW knowledge graph using Three.js. Explore actors, relationships, temporal links, and states in a dynamic force-directed graph. Navigate through the cognitive space with TEM-guided pathfinding.

**Features:**
- Real-time graph updates as new documents are processed
- Color-coded nodes by actor type (Person, Organization, Asset)
- Interactive edges showing relationships and temporal links
- Zoom, pan, and rotate controls for exploration
- Node selection reveals actor details and connected entities

### üöÄ Quick Actions

#### Quick Action Buttons
<img src="assets/images/features/11-quick-action-buttons.png" alt="Quick Action Buttons" width="100%">

One-click access to common operations: new workspace creation, GSW queries, document uploads, VSA verification tests, and TEM navigation demos.

### üìê Full Page Layout

#### Complete Interface Overview
<img src="assets/images/features/12-full-page-layout.png" alt="Full Page Layout" width="100%">

The complete Verridian AI interface showing all panels: chat, settings sidebar, canvas workspace, 3D visualization, and status indicators. Responsive design adapts to different screen sizes while maintaining cognitive workflow efficiency.

### üé® Technical Stack

| Technology | Purpose | Version |
|------------|---------|---------|
| **Next.js** | App Router & Server Components | 16 |
| **React** | UI Framework | 19 |
| **TypeScript** | Type Safety | 5.3+ |
| **Tailwind CSS** | Styling System | 3.4+ |
| **Three.js** | 3D Visualization | Latest |
| **Zustand** | State Management | Latest |
| **Radix UI** | Accessible Components | Latest |

---

## üî¨ Research-Backed Validation (CLAUSE Benchmark)

<div align="center">

### Implementing "Better Call CLAUSE" Benchmark for Australian Family Law

[![CLAUSE Paper](https://img.shields.io/badge/Paper-arXiv:2511.00340-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2511.00340v1)

**üìÑ Full Technical Report**: [CLAUSE Research Application Report](docs/CLAUSE-Research-Application-Report.md)

</div>

Verridian AI integrates advanced validation techniques from the CLAUSE benchmark research to ensure extraction accuracy and statutory compliance.

### CLAUSE Benchmark Framework

```mermaid
flowchart LR
    DOC[Document Input] --> GSW[GSW Extraction]
    GSW --> VAL[Statutory Validation RAG]
    VAL --> EVAL[Multi-Judge Eval]

    GSW --> A[Actors, Roles<br/>States, Links<br/>Questions]
    VAL --> B[FLA 1975<br/>CSAA 1989<br/>FLR 2004]
    EVAL --> C[GPT-4o<br/>Claude<br/>Gemini]
```

### 10-Category Discrepancy Detection

The CLAUSE benchmark introduces comprehensive discrepancy detection adapted for Australian Family Law:

**5 Legal Discrepancies**:
| Category | Description | Family Law Example |
|----------|-------------|-------------------|
| Property Pool Alterations | Incorrect asset valuations | Wrong contribution percentages |
| Parenting Order Contradictions | Conflicting custody arrangements | Inconsistent contact schedules |
| Spousal Maintenance Errors | Wrong income figures | Incorrect duration periods |
| Child Support Calculation Flaws | Income percentage errors | Care percentage conflicts |
| Consent Order Violations | Terms contradicting Family Law Act | Unenforceable provisions |

**5 In-Text Discrepancies**:
| Category | Description | Example |
|----------|-------------|---------|
| Date Inconsistencies | Separation date conflicts | Timeline contradictions |
| Party Name Mismatches | Applicant/Respondent confusion | Children name errors |
| Asset Reference Errors | Property address mismatches | Account number conflicts |
| Numerical Inconsistencies | Dollar amount conflicts | Percentage calculation errors |
| Order Reference Conflicts | Paragraph cross-reference errors | Schedule reference mismatches |

### Validation Features

<table>
<tr>
<td width="50%" valign="top">

#### üéØ Span-Level Detection
Current validation returns span-level precision:
```json
{
  "valid": false,
  "issues": [{
    "type": "numerical_inconsistency",
    "span_start": 38,
    "span_end": 43,
    "flagged_text": "$500k",
    "expected": "$450k",
    "confidence": 0.92,
    "source": "Previous valuation in paragraph 3"
  }]
}
```

</td>
<td width="50%" valign="top">

#### ‚úÖ Statutory RAG Validation
- **Family Law Act 1975 (Cth)**
- **Child Support Assessment Act 1989**
- **Family Law Rules 2004**
- **Federal Circuit Court Rules 2001**
- **Key Family Court Judgments (AustLII)**

</td>
</tr>
</table>

### Multi-Judge Evaluation System

```mermaid
flowchart TB
    RESPONSE[GSW Response] --> GPT[Judge 1<br/>GPT-4o]
    RESPONSE --> CLAUDE[Judge 2<br/>Claude]
    RESPONSE --> GEMINI[Judge 3<br/>Gemini]

    GPT --> AGG[Aggregated Score]
    CLAUDE --> AGG
    GEMINI --> AGG

    AGG --> RESULT[Consensus: 8.0/10<br/>Agreement: 2/3]
```

### Quick Usage Example

```python
from src.validation import StatutoryRAGValidator
from src.benchmarks import FamilyLawDiscrepancyBenchmark
from src.evaluation import MultiJudgeEvaluator

# 1. Validate extraction against statutory corpus
validator = StatutoryRAGValidator("data/statutory_corpus")
result = validator.validate_extraction(
    extraction=gsw_extraction,
    context=original_document
)

# 2. Run discrepancy detection
benchmark = FamilyLawDiscrepancyBenchmark()
discrepancies = benchmark.detect_discrepancies(
    document=court_judgment,
    categories=["payment", "dates", "party_names"]
)

# 3. Multi-judge evaluation
evaluator = MultiJudgeEvaluator(models=["gpt-4o", "claude-sonnet", "gemini-pro"])
scores = evaluator.evaluate(
    extraction=result,
    ground_truth=validated_data
)

print(f"Validation Score: {scores['consensus']:.2f}")
print(f"Confidence: {scores['calibrated_confidence']:.2f}")
print(f"Issues Found: {len(discrepancies)}")
```

**Output:**
```
Validation Score: 0.94
Confidence: 0.89
Issues Found: 2

Discrepancies:
  [1] Payment Term (Line 45-47): Amount mismatch with s79 FLA requirements
  [2] Date Inconsistency (Line 123): Separation date conflicts with filing date
```

---

## üöÄ Quick Start

### üìã Prerequisites

| Requirement | Version | Purpose |
|-------------|---------|---------|
| Python | 3.10+ | Backend runtime |
| Node.js | 18+ | Frontend runtime |
| Git | Latest | Version control |
| OpenRouter API Key | - | LLM access ([get one](https://openrouter.ai)) |

### ‚ö° Installation (5 minutes)

```bash
# 1Ô∏è‚É£ Clone the repository
git clone https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory.git
cd Functional-Structure-of-Episodic-Memory

# 2Ô∏è‚É£ Setup Python environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 3Ô∏è‚É£ Setup Frontend
cd ui && npm install && cd ..

# 4Ô∏è‚É£ Configure environment
cp .env.example .env
# Edit .env and add: OPENROUTER_API_KEY=sk-or-your-key-here

# 5Ô∏è‚É£ Start the UI
cd ui && npm run dev
```

üéâ **Open http://localhost:3000** - You're ready to go!

### üéÆ Demo Scripts

```bash
# Full cognitive system demo
python run_full_system.py

# Individual layer demos
python run_vsa_demo.py      # VSA anti-hallucination
python run_micro_tem.py     # TEM navigation
python run_agent_demo.py    # Active inference
```

---

## üîÑ System Pipeline Overview

<div align="center">

### Complete 6-Step Data Processing Pipeline

</div>

The Verridian AI system follows a **6-step pipeline** to process legal corpora. **Data Labeling is Step 1** - the critical foundation that enables all subsequent processing:

```mermaid
flowchart LR
    subgraph Step1["1Ô∏è‚É£ Data Labeling"]
        LABEL[Domain Classification<br/>11,683 Keywords]
    end

    subgraph Step2["2Ô∏è‚É£ Corpus Setup"]
        DL[Download & Organize<br/>232,560 Documents]
    end

    subgraph Step3["3Ô∏è‚É£ GSW Extraction"]
        GSW[Actor-Centric<br/>Memory Building]
    end

    subgraph Step4["4Ô∏è‚É£ Knowledge Graph"]
        KG[Build Relationships<br/>& Temporal Links]
    end

    subgraph Step5["5Ô∏è‚É£ Verification"]
        VSA[VSA Logic<br/>Anti-Hallucination]
    end

    subgraph Step6["6Ô∏è‚É£ Query Ready"]
        QUERY[Verified Responses<br/>85% Accuracy]
    end

    LABEL --> DL --> GSW --> KG --> VSA --> QUERY
```

| Step | Name | Description | Output |
|------|------|-------------|--------|
| **1** | **Data Labeling** | Classify documents by legal domain | 21 domain-specific files |
| **2** | Corpus Setup | Download & organize raw corpus | 232,560 documents ready |
| **3** | GSW Extraction | Extract actors, states, relationships | Actor-centric memory |
| **4** | Knowledge Graph | Build spatio-temporal links | Connected knowledge base |
| **5** | Verification | VSA anti-hallucination layer | Validated facts |
| **6** | Query Ready | System ready for queries | 85% F1 accuracy |

---

## 1Ô∏è‚É£ Step 1: Data Labeling (Domain Classification)

### üì• Download the Corpus

The corpus is available from the **UMARV-FoE/Open-Australian-Legal-Corpus** on Hugging Face.

**Option A: Using Hugging Face CLI (Recommended)**

```bash
# 1Ô∏è‚É£ Install Hugging Face CLI
pip install huggingface_hub

# 2Ô∏è‚É£ Login to Hugging Face (free account required)
huggingface-cli login

# 3Ô∏è‚É£ Download the corpus (8.8 GB - may take 30+ minutes)
huggingface-cli download UMARV-FoE/Open-Australian-Legal-Corpus \
    --local-dir ./corpus-download \
    --repo-type dataset
```

**Option B: Direct Download**

```bash
wget https://huggingface.co/datasets/UMARV-FoE/Open-Australian-Legal-Corpus/resolve/main/corpus.jsonl
```

**Option C: Python Script**

```python
from huggingface_hub import hf_hub_download

file_path = hf_hub_download(
    repo_id="UMARV-FoE/Open-Australian-Legal-Corpus",
    filename="corpus.jsonl",
    repo_type="dataset",
    local_dir="./corpus-download"
)
print(f"Downloaded to: {file_path}")
```

### üéØ Run Domain Classification

**üìÑ Full Technical Documentation**: [Corpus Classification Pipeline Wiki](wiki/Corpus-Classification-Pipeline.md)

**Step 1 is the critical data labeling step** that organizes all 232,560 legal documents into semantic domains. This must be completed before GSW extraction.

```bash
# Run domain extraction (streaming - RAM safe for 9.4GB corpus)
python -m src.ingestion.corpus_domain_extractor \
    --input data/corpus.jsonl \
    --output data/processed/domains \
    --progress 5000

# Output: Creates 21 domain-specific JSONL files
# data/processed/domains/
# ‚îú‚îÄ‚îÄ family.jsonl         (Family Law documents)
# ‚îú‚îÄ‚îÄ criminal.jsonl       (Criminal Law documents)
# ‚îú‚îÄ‚îÄ property.jsonl       (Property Law documents)
# ‚îî‚îÄ‚îÄ ... (21 domain files total)
```

### üìä Classification Statistics

| Metric | Count |
|--------|-------|
| **Total Keywords** | 11,683 |
| **Legal Categories** | 86 |
| **Primary Domains** | 21 |
| **Domain Knowledge Files** | 12 |
| **Comprehensive Dictionaries** | 5 |
| **Legislation Patterns** | 200+ |
| **Case Citation Patterns** | 150+ |

### üß† Multi-Dimensional Classification Approach

The classifier uses **four orthogonal dimensions** to achieve high-accuracy domain assignment:

```mermaid
flowchart TB
    DOC[Legal Document] --> KW[Keyword Analysis<br/>11,683 domain-specific terms]
    DOC --> LEG[Legislation Extraction<br/>200+ statute patterns]
    DOC --> CASE[Case Citation Parsing<br/>150+ court citation formats]
    DOC --> COURT[Court Hierarchy<br/>Jurisdictional authority]

    KW --> BOOST[5-Factor Boost Scoring]
    LEG --> BOOST
    CASE --> BOOST
    COURT --> BOOST

    BOOST --> DOMAIN[Primary Domain Assignment<br/>+ Confidence Score]
    BOOST --> META[Enhanced Metadata<br/>Court Level, Authority Score]
```

### üî¨ Research Phase: Domain Knowledge Development

The classification system was built through extensive research across **12 specialized legal domains**:

<table>
<tr>
<td width="50%" valign="top">

#### Primary Legal Domains
| Domain | Keywords | Coverage |
|--------|----------|----------|
| **Family Law** | 1,847 | Comprehensive |
| **Criminal Law** | 1,523 | Comprehensive |
| **Property Law** | 1,289 | Comprehensive |
| **Commercial Law** | 1,156 | Comprehensive |
| **Employment Law** | 1,078 | Comprehensive |
| **Administrative Law** | 956 | Comprehensive |

</td>
<td width="50%" valign="top">

#### Specialized Domains
| Domain | Keywords | Coverage |
|--------|----------|----------|
| **Tax Law** | 734 | Comprehensive |
| **Constitutional Law** | 612 | Comprehensive |
| **Equity & Trusts** | 589 | Comprehensive |
| **Torts** | 567 | Comprehensive |
| **Evidence & Procedure** | 543 | Comprehensive |
| **Resources/Energy** | 489 | Comprehensive |

</td>
</tr>
</table>

### üìÅ Research Documentation

The domain knowledge research is documented in specialized files:

```
üìÅ Domain Knowledge Research
‚îú‚îÄ‚îÄ ADMINISTRATIVE_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ COMMERCIAL_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ CONSTITUTIONAL_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ CRIMINAL_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ EMPLOYMENT_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ EQUITY_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ EVIDENCE_PROCEDURE_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ FAMILY_LAW_ACT_1975_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ PROPERTY_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ RESOURCES_INFRASTRUCTURE_ENERGY_LAW_DOMAIN_KNOWLEDGE.md
‚îú‚îÄ‚îÄ TAX_LAW_DOMAIN_KNOWLEDGE.md
‚îî‚îÄ‚îÄ TORTS_LAW_DOMAIN_KNOWLEDGE.md

üìÅ Comprehensive Keyword Dictionaries
‚îú‚îÄ‚îÄ AUSTRALIAN_COMMERCIAL_LAW_COMPLETE_DICTIONARY.py
‚îú‚îÄ‚îÄ AUSTRALIAN_HEALTH_MEDICAL_REGULATORY_LAW_COMPREHENSIVE.py
‚îú‚îÄ‚îÄ AUSTRALIAN_SPECIALIZED_NICHE_LAW_DOMAINS.py
‚îú‚îÄ‚îÄ COMPREHENSIVE_COMMERCIAL_LAW_DICTIONARY.py
‚îî‚îÄ‚îÄ resources_infrastructure_energy_law_dict.py
```

### ‚ö° 5-Factor Boost Scoring System

Each document receives a multi-factor score combining all classification dimensions:

| Factor | Weight | Description |
|--------|--------|-------------|
| **Keyword Density** | 40% | Domain-specific term frequency |
| **Legislation Alignment** | 25% | Statutes cited match domain |
| **Case Authority** | 15% | Precedents align with domain |
| **Court Level** | 10% | Jurisdictional weighting |
| **Cross-Reference** | 10% | Inter-domain relationship scoring |

### üèó Implementation Files

| File | Purpose |
|------|---------|
| `src/ingestion/classification_config.py` | 11,683 keywords in 86 categories |
| `src/ingestion/legislation_patterns.py` | Statute recognition patterns |
| `src/ingestion/case_patterns.py` | Case citation parsing |
| `src/ingestion/court_hierarchy.py` | Australian court taxonomy |
| `src/ingestion/corpus_domain_extractor.py` | Multi-dimensional classifier |
| `scripts/integrate_keywords.py` | Keyword aggregation tool |

### üìà Classification Output

Each classified document includes rich metadata:

```json
{
  "citation": "[2023] FamCAFC 123",
  "text": "...",
  "_classification": {
    "primary_domain": "Family",
    "primary_category": "family_property",
    "all_matches": [["family_property", 156], ["family_children", 89]],
    "court": "Family Court of Australia (Full Court)",
    "court_level": "appellate",
    "authority_score": 0.85,
    "legislation_refs": ["Family Law Act 1975", "Child Support Act 1988"],
    "case_refs": ["Stanford v Stanford", "Mallet v Mallet"]
  }
}
```

> **Data Quality**: This classification system demonstrates professional-grade data preparation, achieving domain assignment for 232,560 documents in ~15 hours with streaming memory efficiency.

---

## 3Ô∏è‚É£ Step 3: GSW Extraction & Cognitive Processing

After Step 1 classification, documents are ready for GSW (Global Semantic Workspace) extraction - the cognitive processing that builds actor-centric memory.

### üéØ Run GSW Extraction

```bash
# ‚ö†Ô∏è Requires OPENROUTER_API_KEY in .env file

# Start with a small batch to verify setup
python scripts/gsw_pipeline.py process --domain family --limit 10

# Process 100 documents (~$1-2 cost)
python scripts/gsw_pipeline.py process --domain family --limit 100

# Process full domain (costs vary by LLM choice)
python scripts/gsw_pipeline.py process --domain family
```

### üìä Analysis & Reports (Steps 4-6)

```bash
# Step 4: Build knowledge graph
python scripts/gsw_pipeline.py analyze

# Step 5: Verification is automatic via VSA layer

# Step 6: Query ready - run summary
python scripts/gsw_pipeline.py summary --domain family
```

> **Note**: GSW extraction requires LLM API calls. See the [LLM Model Comparison](#-llm-model-comparison-pricing-quality--toon-savings) section for cost estimates.

---

## üìä Performance

<div align="center">

### Benchmark Results

```mermaid
graph LR
    subgraph Verridian["Verridian AI"]
        V1[85% Accuracy]
        V2[3,500 tokens]
        V3[11.83ms response]
    end

    subgraph Traditional["Traditional RAG"]
        T1[77% Accuracy]
        T2[8,000 tokens]
        T3[~500ms response]
    end

    V1 -.-|+10%| T1
    V2 -.-|56% less| T2
    V3 -.-|42x faster| T3
```

</div>

| Metric | Verridian | Traditional RAG | Improvement |
|--------|-----------|-----------------|-------------|
| **Accuracy** | 85% | 77% | +10% |
| **Token Usage** | ~3,500/query | ~8,000/query | 56% reduction |
| **Response Time** | 11.83ms | ~500ms | 42x faster |
| **Query Success** | 100% | ~95% | +5% |

### üìà Corpus & Knowledge Base Statistics

| Metric | Count |
|--------|-------|
| **Total Legal Documents** | 232,560 |
| **Legal Domains** | 21 |
| **Classification Keywords** | 11,683 |
| **Python LOC** | 14,549 |
| **Documentation Pages** | 25+ |

---

## üí∞ LLM Model Comparison: Pricing, Quality & TOON Savings

Understanding which LLM to use for GSW extraction is critical for both cost and quality.

### üìä 2025 Model Pricing Comparison

```mermaid
flowchart LR
    subgraph Budget["üíµ Budget Tier"]
        direction TB
        GLM[GLM-4.6<br/>$0.60/$2.00]
        KIMI[Kimi K2<br/>$0.15/$2.50]
        MINI[MiniMax M2<br/>$0.30/$1.20]
    end

    subgraph Mid["‚öñÔ∏è Mid Tier"]
        direction TB
        FLASH[Gemini 2.5 Flash<br/>$0.30/$2.50]
        GPT[GPT-4.1<br/>$2/$8]
    end

    subgraph Premium["üèÜ Premium Tier"]
        direction TB
        CLAUDE[Claude Sonnet 4.5<br/>$3/$15]
        PRO[Gemini 2.5 Pro<br/>$1.25/$10]
    end

    Budget -->|Lower Cost<br/>Good Quality| Mid
    Mid -->|Higher Cost<br/>Better Quality| Premium
```

### API Pricing Per Million Tokens (November 2025)

| Model | Input Cost | Output Cost | Context Window | Best For |
|-------|------------|-------------|----------------|----------|
| **[Gemini 2.5 Flash](https://ai.google.dev/gemini-api/docs/pricing)** | $0.30 | $2.50 | 1M tokens | High-volume processing |
| **[Gemini 2.5 Flash-Lite](https://ai.google.dev/gemini-api/docs/pricing)** | $0.10 | $0.40 | 1M tokens | Bulk extraction (lowest cost) |
| **[Gemini 2.5 Pro](https://ai.google.dev/gemini-api/docs/pricing)** | $1.25 | $10.00 | 1M tokens | Complex legal reasoning |
| **[GPT-4.1](https://openai.com/api/pricing/)** | $2.00 | $8.00 | 1M tokens | General purpose, reliable |
| **[Claude Sonnet 4.5](https://docs.claude.com/en/docs/about-claude/pricing)** | $3.00 | $15.00 | 200K tokens | Best coding/legal accuracy |
| **[Kimi K2](https://platform.moonshot.ai/docs/pricing/chat)** | $0.15* | $2.50 | 128K tokens | Cost-sensitive applications |
| **[GLM-4.6](https://open.bigmodel.cn/pricing)** | $0.60 | $2.00 | 128K tokens | Open weights, self-hosting |
| **[MiniMax M2](https://www.minimaxi.com/news/minimax-m2)** | $0.30 | $1.20 | 128K tokens | Tool calling, agents |

*Kimi K2 input price is $0.15 with cache hit, $0.60 without cache.

### üíé Full Corpus GSW Extraction Cost Estimates

Processing all **232,560 documents** (Phase 3 GSW extraction):

| Model | Cost (With TOON) | Cost (Without TOON) | TOON Saves |
|-------|------------------|---------------------|------------|
| **Gemini 2.5 Flash-Lite** | **$372** | $528 | $156 |
| **MiniMax M2** | **$1,012** | $1,244 | $232 |
| **Kimi K2 (cached)** | **$1,833** | $2,326 | $493 |
| **Gemini 2.5 Flash** | **$1,919** | $2,540 | $621 |
| **GLM-4.6** | **$1,744** | $2,326 | $582 |
| **GPT-4.1** | **$6,743** | $8,683 | $1,940 |
| **Gemini 2.5 Pro** | **$7,704** | $10,517 | $2,813 |
| **Claude Sonnet 4.5** | **$12,208** | $16,277 | $4,069 |

> **Note**: Using Gemini 2.5 Flash-Lite with TOON format, the full corpus GSW extraction costs **under $400**! Phase 2 classification is free (no API calls).

### üèÜ Recommended Models by Use Case

| Use Case | Recommended Model | Why |
|----------|-------------------|-----|
| **Proof of Concept** | Gemini 2.5 Flash | Balance of cost & quality |
| **Budget Processing** | Gemini 2.5 Flash-Lite | Lowest cost per document |
| **Highest Accuracy** | Claude Sonnet 4.5 | Best legal reasoning |
| **Self-Hosted** | GLM-4.6 | Open weights, no API costs |
| **High Volume** | Kimi K2 (cached) | Cache reduces costs 75% |
| **Production** | Gemini 2.5 Pro | Quality + reasonable cost |

---

## üìñ Documentation

<div align="center">

### üìö [Full Documentation on Wiki](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki)

</div>

<table>
<tr>
<td width="50%" valign="top">

### üèó Architecture
- [Architecture Overview](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Architecture-Overview)
- [Three-Layer System](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Three-Layer-System)
- [GSW Workspace](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/GSW-Global-Semantic-Workspace)
- [Data Flow](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Data-Flow)

### ‚öôÔ∏è Backend Modules
- [GSW Module](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Backend-GSW-Module)
- [TEM Module](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Backend-TEM-Module)
- [VSA Module](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Backend-VSA-Module)
- [Agency Module](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Backend-Agency-Module)

</td>
<td width="50%" valign="top">

### üñ• Frontend
- [Frontend Overview](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Frontend-Overview)
- [API Routes](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Frontend-API-Routes)
- [Components](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Frontend-Components)

### üìò Guides & Reference
- [Quick Start](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Quick-Start)
- [API Reference](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/API-Reference)
- [Data Schemas](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Data-Schemas)
- [Glossary](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/wiki/Glossary)

</td>
</tr>
</table>

---

## üóÇ Project Structure

```mermaid
graph TB
    subgraph Root["üì¶ Functional-Structure-of-Episodic-Memory"]
        direction TB

        subgraph Backend["üêç src/ - Python Backend (14,549 LOC)"]
            direction TB
            GSW[gsw/<br/>Global Semantic Workspace]
            TEM[tem/<br/>Tolman-Eichenbaum Machine]
            VSA_DIR[vsa/<br/>Vector Symbolic Architecture]
            AGENCY[agency/<br/>Active Inference]
            AGENTS[agents/<br/>LangChain Tools]
            INGEST[ingestion/<br/>Document Processing]
            LOGIC[logic/<br/>Schemas & Rules]
        end

        subgraph Frontend["üåê ui/ - Next.js 16 Frontend"]
            direction TB
            APP[src/app/<br/>App Router]
            COMP[src/components/<br/>React Components]
            LIB[src/lib/<br/>TypeScript Libraries]
        end

        subgraph Data["üìä data/ - Knowledge Base"]
            direction TB
            WS[workspaces/<br/>GSW Snapshots]
            BENCH[benchmarks/<br/>Test Data]
            LEG[legislation/<br/>Family Law Act]
        end

        WIKI[üìö wiki/<br/>25+ Doc Pages]
        ASSETS[üñº assets/<br/>Visual Assets]
        TESTS[üß™ tests/<br/>Test Suite]
    end

    GSW --> |legal_operator.py| TEM
    TEM --> |model.py| VSA_DIR
    VSA_DIR --> |legal_vsa.py| AGENCY
    AGENCY --> |agent.py| AGENTS
```

### üìÅ Detailed File Structure

| Directory | Key Files | Purpose |
|-----------|-----------|---------|
| **src/gsw/** | `legal_operator.py`, `workspace.py`, `text_chunker.py` | 6-task extraction pipeline, persistence |
| **src/tem/** | `model.py`, `action_space.py` | PyTorch TEM, legal action definitions |
| **src/vsa/** | `legal_vsa.py`, `ontology.py` | Hyperdimensional logic, legal rules |
| **src/agency/** | `agent.py`, `generative_model.py` | POMDP agent, A/B/C/D matrices |
| **src/agents/** | Various tools | LangChain integration |
| **src/ingestion/** | Operators, classifiers | Document processing |
| **src/logic/** | Schemas | Pydantic models |
| **ui/src/app/** | `page.tsx`, `api/` routes | Chat interface, visualizations |
| **ui/src/components/** | React components | UI building blocks |
| **data/** | JSON workspaces | Knowledge base storage |

---

## üî¨ Research

<div align="center">

This project implements research from:

[![arXiv](https://img.shields.io/badge/arXiv-2511.07587-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2511.07587)

**"Functional Structure of Episodic Memory"**

</div>

### Foundational Research Papers

| Theory | Paper | Authors |
|--------|-------|---------|
| **Tolman-Eichenbaum Machine** | [The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory](https://www.cell.com/cell/fulltext/S0092-8674(20)31388-X) | Whittington et al., 2020 |
| **Clone-Structured Cognitive Graphs** | [Clone-structured graph representations enable flexible learning](https://www.biorxiv.org/content/10.1101/770495v2.full.pdf) | George et al., 2021 |
| **Active Inference** | [Active Inference: A Process Theory](https://www.fil.ion.ucl.ac.uk/~karl/Active%20Inference%20A%20Process%20Theory.pdf) | Friston et al., 2017 |
| **Hyperdimensional Computing** | [Hyperdimensional Computing: An Introduction](https://www.rctn.org/vs265/kanerva09-hyperdimensional.pdf) | Kanerva, 2009 |
| **Global Workspace Theory** | [Global Workspace Theory of Consciousness](https://tilde.ini.uzh.ch/~kiper/Baars_1.pdf) | Baars, 1997 |

---

## ü§ù Call for Contributors

**I'm looking for contributors who share my vision: achieving 100% accuracy in cognitive retrieval.**

Current AI systems hallucinate, miss connections, and fail to understand context. The architecture in this repository is a significant step toward solving these problems, but there's more work to be done:

| Area | What We Need |
|------|--------------|
| **Entity Resolution** | Improving entity matching across documents |
| **Domain Expansion** | Adapting to medical, business, and other domains |
| **Algorithm Optimization** | Enhancing cognitive navigation performance |
| **Evaluation Frameworks** | Building better benchmark suites |
| **Data Integration** | Supporting additional data sources |

**If you believe AI should truly understand information rather than just pattern-match against it, join me.**

See our [Contributing Guidelines](CONTRIBUTING.md) for how to get involved.

```bash
# Quick contribution workflow
git checkout -b feature/your-feature
# Make changes
pytest tests/
git commit -m "feat(module): description"
git push origin feature/your-feature
# Open Pull Request
```

---

## üìú License

<div align="center">

MIT License - see [LICENSE](LICENSE) for details.

</div>

---

## üîÆ Future Research: BRAINS

> **Coming Soon: BRAINS (Bio-Inspired Regulatory AI Neural System)**
>
> I am developing a novel architecture for AI safety inspired by the regulatory mechanisms observed in the fruit fly (*Drosophila*) brain.
>
> ### The Insight: Neuromodulation as Regulation
>
> The key insight comes from how biological brains regulate behavior. When a fruit fly experiences a stimulus, it doesn't merely process the information locally. Specialized neurons release neuromodulators (such as dopamine or octopamine). These chemicals deliver widespread valence signals‚Äîpositive or negative‚Äîthat globally influence neural activity and synaptic plasticity. This system-wide feedback allows the organism to rapidly adapt its behavior based on experience.
>
> ### The Role of Neurochemistry in Behavior
>
> In humans, complex processes like morality and ethical decision-making are similarly underpinned by intricate neurochemical pathways. We don't make decisions based purely on abstract logic; our choices are deeply intertwined with physiological feedback loops. This interplay between cognition and neurochemistry is fundamental to how we learn and regulate behavior within social and ethical constraints.
>
> ### Simulating Neuromodulation in AI
>
> **What if we could adapt these biological principles for AI alignment?**
>
> The BRAINS system utilizes a simulated neuromodulation architecture designed to:
>
> - **Monitor Internal States**: Observe network activity and latent representations across all layers, not just the final output
> - **Implement Global Feedback**: Introduce system-wide reinforcement signals based on the quality, safety, and truthfulness of the AI's processes and outputs
> - **Dynamic Regulation**: Apply positive modulation (strengthening pathways) when the AI adheres to behavioral constraints, and negative modulation (dampening pathways) when it deviates
> - **Safety Integration**: Incorporate fail-safe protocols, including behavioral suppression or system shutdown, if the AI deviates into unsafe patterns
>
> ### A New Layer of Alignment
>
> Rather than relying exclusively on training data and prompting for alignment, BRAINS introduces a dynamic, system-wide regulatory mechanism analogous to the biological systems that shape adaptive behavior. This approach aims to provide an intrinsic feedback loop for regulating AI actions in real-time.
>
> This work builds on the cognitive architecture in this repository, adding a crucial regulatory layer inspired by neuroscience to advance the development of safe, reliable, and truthful AI. I look forward to releasing a detailed research paper outlining the architecture and initial findings in the coming months.

---

<div align="center">

### Created by Daniel Fleuren | [Verridian AI](https://github.com/Verridian-ai)

*Building Cognitive AI That Actually Understands*

<br>

[![GitHub](https://img.shields.io/badge/GitHub-Verridian--ai-181717?style=for-the-badge&logo=github)](https://github.com/Verridian-ai)
[![Paper](https://img.shields.io/badge/Paper-arXiv-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2511.07587)
[![Issues](https://img.shields.io/badge/Issues-Report-red?style=for-the-badge&logo=github)](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/issues)
[![Discussions](https://img.shields.io/badge/Discussions-Join-purple?style=for-the-badge&logo=github)](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/discussions)

---

**Open to collaboration, research partnerships, and contributors who want to push the boundaries of what AI can understand.**

**Full Corpus Pipeline** ‚Ä¢ Production-ready Architecture ‚Ä¢ 232,560 Legal Documents

<br>

> *"The brain doesn't search for memories - it reconstructs them."*
>
> ‚Äî Cognitive Neuroscience Principle

*Contact: [GitHub Issues](https://github.com/Verridian-ai/Functional-Structure-of-Episodic-Memory/issues) or contribute directly via Pull Request*

</div>
