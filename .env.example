# Verridian Legal AI - Environment Configuration
# ==============================================
# Copy this file to .env and fill in your values

# ============================================
# LLM API Configuration (Required)
# ============================================

# OpenRouter API Key (get from https://openrouter.ai)
# This is your primary API key for accessing AI models through OpenRouter
# Used for: Chat completions, text generation, and AI-powered features
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# Optional: Direct Google AI access
# Only needed if you want to bypass OpenRouter for Google models
# GOOGLE_AI_API_KEY=your-google-ai-key

# ============================================
# Client-Side API Keys (Public Variables)
# ============================================

# Next.js Public OpenRouter API Key
# Used for client-side requests (e.g., image generation)
# NOTE: Only use this for non-sensitive operations as it's exposed to the browser
# Falls back to OPENROUTER_API_KEY if not set
NEXT_PUBLIC_OPENROUTER_API_KEY=sk-or-v1-your-public-key-here

# Next.js Public App URL
# The base URL of your application (used for HTTP-Referer headers)
# Development: http://localhost:3000
# Production: https://yourdomain.com
NEXT_PUBLIC_APP_URL=http://localhost:3000

# ============================================
# AI Memory & Context (Mem0 Integration)
# ============================================

# Mem0 API Key (get from https://mem0.ai)
# Enables persistent memory across chat sessions
# Used for: Storing and retrieving conversation context, user preferences
# Optional - chat will work without it, but won't have long-term memory
MEM0_API_KEY=your-mem0-api-key-here

# ============================================
# LangFuse Observability (Local Self-Hosted)
# ============================================

# After running docker compose up -d in docker/langfuse:
# 1. Go to http://localhost:3001
# 2. Create an account
# 3. Create a project
# 4. Get API keys from Settings > API Keys

LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key
LANGFUSE_HOST=http://localhost:3001

# ============================================
# Data Paths
# ============================================

DATA_DIR=./data
CORPUS_DIR=./data/corpus
PROCESSED_DIR=./data/processed

# ============================================
# Model Configuration
# ============================================

# Embedding model (for vector search)
EMBEDDING_MODEL=BAAI/bge-m3

# LLM model (via OpenRouter)
# Gemini 2.0 Flash Experimental - FREE model, 1M context window
LLM_MODEL=google/gemini-2.0-flash-exp:free

# Temperature for generation
LLM_TEMPERATURE=0.7

# ============================================
# System Configuration
# ============================================

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Enable debug mode
DEBUG=false

# Maximum concurrent LLM requests
MAX_CONCURRENT_REQUESTS=5

# ============================================
# Optional: Production Settings
# ============================================

# For production deployment:
# NODE_ENV=production
# NEXTAUTH_SECRET=your-production-secret-min-32-chars
# DATABASE_URL=postgresql://user:pass@host:port/db

# ============================================
# Setup Instructions
# ============================================
#
# 1. Copy this file: cp .env.example .env
# 2. Get OpenRouter API key from https://openrouter.ai
# 3. (Optional) Get Mem0 API key from https://mem0.ai for memory features
# 4. (Optional) Set up LangFuse for observability (see docker/langfuse)
# 5. Update NEXT_PUBLIC_APP_URL when deploying to production
# 6. Never commit your .env file to version control!
